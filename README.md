# ğŸ“š Book Recommendation System (Built from Scratch)

---

## ğŸš€ Project Overview

This project implements a **Book Recommendation System** built **entirely from scratch** using **Collaborative Filtering** techniques.

The primary objective is to design an **end-to-end machine learning system** that reflects how recommender systems are built in real-world production environments â€” not just as a notebook experiment, but as a structured, extensible ML pipeline.

This project intentionally focuses on:

* System design and modular ML pipelines
* Real-world data challenges such as sparsity and cold start
* Reproducibility and evaluation
* Transparent progress tracking
* Explicit demonstration of technical skills

---

## ğŸ¯ Problem Statement

Given historical userâ€“book rating interactions, predict which books a user is most likely to enjoy next.

Key challenges addressed:

* Highly sparse userâ€“item interaction data
* Cold-start users and books
* Similarity computation at scale
* Meaningful evaluation beyond simple accuracy metrics

---

## ğŸ§  Recommendation Approach

The system is based on **Collaborative Filtering**, where recommendations are generated from patterns in user behavior rather than book metadata.

Approaches implemented / planned:

* **Userâ€“User Collaborative Filtering**
* **Itemâ€“Item Collaborative Filtering**

This mirrors real-world recommender systems used by platforms such as e-commerce, media streaming, and content discovery services.

---

## ğŸ›  How This System Is Built (Technical Approach)

This project is developed incrementally using a production-style workflow.

### 1. Data Ingestion

* Loaded raw CSV datasets into a structured raw data layer
* Performed schema validation and basic sanity checks

### 2. Data Preprocessing

* Removed invalid and missing ratings
* Normalized rating values
* Filtered inactive users and low-frequency books to reduce noise

### 3. Feature Engineering

* Constructed sparse userâ€“item matrices
* Applied normalization techniques to stabilize similarity calculations

### 4. Modeling

* Implemented Userâ€“User and Itemâ€“Item Collaborative Filtering
* Experimented with cosine similarity and Pearson correlation
* Tuned neighbor selection to balance recommendation quality and performance

### 5. Evaluation

* Used RMSE and MAE to evaluate rating prediction accuracy
* Used Precision@K and Recall@K to evaluate recommendation relevance

### 6. Inference

* Built reusable functions for Top-N recommendation generation
* Designed logic to handle unseen users and books (cold start handling)

Each component is implemented modularly to support future production deployment.

---

## ğŸ“Š Dataset

The project uses the **Book-Crossing Dataset**, a publicly available dataset containing explicit user ratings for books.

Dataset components:

* Users (`User-ID`)
* Books (`ISBN`)
* Ratings (`Book-Rating`, scale 0â€“10)

Directory structure:

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ books.csv
â”‚   â”œâ”€â”€ users.csv
â”‚   â””â”€â”€ ratings.csv
â””â”€â”€ processed/
```

---

## ğŸ—ï¸ System Architecture (Planned)

```
Data Ingestion
     â†“
Data Validation & Cleaning
     â†“
Exploratory Data Analysis
     â†“
Userâ€“Item Matrix Construction
     â†“
Collaborative Filtering Models
     â†“
Model Evaluation
     â†“
Recommendation Engine
     â†“
API / Dashboard (Phase 2)
```

---

## ğŸ›  Tech Stack

| Layer                | Tools                                  |
| -------------------- | -------------------------------------- |
| Language             | Python                                 |
| Data Processing      | Pandas, NumPy                          |
| Modeling             | Scikit-learn, SciPy                    |
| Similarity Metrics   | Cosine Similarity, Pearson Correlation |
| Evaluation           | RMSE, MAE, Precision@K, Recall@K       |
| Visualization        | Matplotlib, Seaborn                    |
| API (Planned)        | FastAPI                                |
| Deployment (Planned) | Docker                                 |

---

## ğŸ§© Skills Demonstrated

* Data ingestion and preprocessing (Pandas, NumPy)
* Handling sparse real-world datasets
* Similarity-based machine learning algorithms
* Feature engineering for recommender systems
* Model evaluation using ranking-based metrics
* Modular ML pipeline design
* Clear technical documentation and progress tracking

---

## ğŸ“Œ Project Progress Tracker

### Phase 1: Foundation & Data
---

### Phase 2: Exploratory Data Analysis
---

### Phase 3: Feature Engineering
---

### Phase 4: Collaborative Filtering Models
---

### Phase 5: Evaluation Framework
---

### Phase 6: Recommendation Engine
---

### Phase 7: Productionization (Planned â€“ Level 2)
---

### Phase 8: Visualization & Dashboard (Optional)

---

## ğŸ§ª Evaluation Metrics

| Metric      | Purpose                            |
| ----------- | ---------------------------------- |
| Precision@K | Relevance of top-K recommendations |
| Recall@K    | Coverage of relevant items         |

Latest notebook results (Section 7: Evaluation, using uv-managed env):

| Metric        | Value   | Notes                              |
| ------------- | ------- | ---------------------------------- |
| Precision@10  | 0.0403  | Item-based KNN, cosine similarity |
| Recall@10     | 0.1151  | Item-based KNN, cosine similarity |

---

## â–¶ï¸ How to Run (Initial Setup)

```bash
git clone <your-repository-url>
cd book-recommendation-system
python -m venv venv
source venv/bin/activate    # macOS / Linux
venv\Scripts\activate       # Windows
pip install -r requirements.txt
```

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ Data/                      # Raw CSVs (books, users, ratings)
â”œâ”€â”€ artifacts/                 # Generated artifacts (cleaned CSVs, pivot, model, etc.)
â”œâ”€â”€ notebooks/                 # EDA, modelling, evaluation notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/            
    â”œâ”€â”€data_ingestion.py
    â”œâ”€â”€data_preprocessing.py
    â”œâ”€â”€model_preparation.py
â””â”€â”€ pipelines/             
    â”œâ”€â”€data_pipeline.py
    â”œâ”€â”€training_pipeline.py
    â”œâ”€â”€prediction_pipeline.py
â”œâ”€â”€ app.py                     # FastAPI service
â”œâ”€â”€ streamlit_app.py           # Streamlit UI
â”œâ”€â”€ Dockerfile                 # uv-based container image
â”œâ”€â”€ docker-compose.yml         # Orchestration for data/train/api/ui
â”œâ”€â”€ ARCHITECTURE.md            # System architecture and diagrams
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml             # uv/PEP 621 metadata + deps
â””â”€â”€ requirements.txt
```

---

## âœ¨ What Makes This Project Different

* Built completely from scratch without starter templates
* Focused on ML system design, not just model accuracy
* Uses ranking-based evaluation metrics
* Tracks development progress transparently
* Designed to evolve into a production ML system

---

## ğŸ§  Interview Talking Points

* Built an end-to-end recommender system from scratch
* Designed for sparse, real-world user interaction data
* Evaluated recommendations using both accuracy and ranking metrics
* Structured the project for scalability and production deployment
* Demonstrated ML engineering decision-making

---

## ğŸ‘¤ Author

**Your Name**
Machine Learning Engineer / Applied Machine Learning
LinkedIn | GitHub

---

## â­ Project Status

ğŸš§ **Actively in Development â€” progress tracked in this README**

---

## âš¡ Quick Start (Docker + uv)

Prereqs: Docker + Docker Compose.

1) Build images  
`docker compose build`

2) Generate artifacts (ingestion â†’ preprocessing â†’ training)  
`docker compose run --rm train`
   - Artifacts land in a named volume mounted at `/app/artifacts`.

3) Run API and UI  
`docker compose up api ui`
   - API: http://localhost:18000 (health at `/health`, recommend at `/recommend`)  
   - UI:  http://localhost:18001 (talks to the API automatically)

4) Call the API directly  
`curl -X POST http://localhost:18000/recommend -H "Content-Type: application/json" -d '{"book": "A Bend in the Road"}'`

5) Query via CLI (inside repo, using existing artifacts)  
`python -m src.pipelines.prediction_pipeline --book "A Bend in the Road"`

6) Clean up containers/volumes  
`docker compose down`  
`docker volume rm book-recommendation-system-ml-project_artifacts` (only if you want to drop saved artifacts)

Notes:
- The Streamlit UI defaults to the mapped API endpoint; override with `API_URL` if the API runs elsewhere.
- If a title isnâ€™t in the pivot (filtered catalog), the API returns 404 with a clear message.
